{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual counts: [('spam', 10), ('ham', 10)]\n",
      "Predicted counts: [('spam', 11), ('ham', 9)]\n",
      "             Predicted:    \n",
      "                   spam ham\n",
      "Actual: spam          5   5\n",
      "        ham           6   4\n",
      "Accuracy: 0.45\n",
      "Manually computed accuracy: 0.45\n",
      "Precision: 0.45\n",
      "Manually computed precision: 0.45\n",
      "Recall: 0.5\n",
      "Manually computed recall: 0.5\n",
      "F1 score: 0.48\n",
      "Manually computed F1 score: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\opencv\\lib\\site-packages\\ipykernel_launcher.py:42: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "c:\\users\\user\\opencv\\lib\\site-packages\\ipykernel_launcher.py:45: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created July 2017\n",
    "\n",
    "@author: arw\n",
    "\"\"\"\n",
    "\n",
    "# Understanding Machine Learning Evaluation metrics using a 'confusion matrix'\n",
    "# We use sklearn's metrics method and compare results with what we expect by calculating\n",
    "# the metrics manually\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# First some dummy data for the metric methods to compute the metrics\n",
    "actual_labels = ['spam', 'ham', 'spam', 'spam', 'spam',\n",
    "               'ham', 'ham', 'spam', 'ham', 'spam',\n",
    "               'spam', 'ham', 'ham', 'ham', 'spam',\n",
    "               'ham', 'ham', 'spam', 'spam', 'ham']\n",
    "              \n",
    "predicted_labels = ['spam', 'spam', 'spam', 'ham', 'spam',\n",
    "                    'spam', 'ham', 'ham', 'spam', 'spam',\n",
    "                    'ham', 'ham', 'spam', 'ham', 'ham',\n",
    "                    'ham', 'spam', 'ham', 'spam', 'spam']\n",
    "\n",
    "# Computer the cells of the confusion matrix                    \n",
    "ac = Counter(actual_labels)                     \n",
    "pc = Counter(predicted_labels)  \n",
    "\n",
    "print('Actual counts:', ac.most_common())\n",
    "print('Predicted counts:', pc.most_common())         \n",
    "        \n",
    "# Define confusion matrix using sklearn function\n",
    "cm = metrics.confusion_matrix(y_true=actual_labels,\n",
    "                         y_pred=predicted_labels,\n",
    "                         labels=['spam','ham'])\n",
    "print(pd.DataFrame(data=cm, \n",
    "                   columns=pd.MultiIndex(levels=[['Predicted:'],\n",
    "                                                 ['spam','ham']], \n",
    "                                         labels=[[0,0],[0,1]]), \n",
    "                   index=pd.MultiIndex(levels=[['Actual:'],\n",
    "                                               ['spam','ham']], \n",
    "                                       labels=[[0,0],[0,1]])))\n",
    "\n",
    "# We need to declare what is positive - other will be taken as negative                                      \n",
    "positive_class = 'spam'\n",
    "\n",
    "# We first calculate the metric 'accuracy' using metric function\n",
    "accuracy = np.round(\n",
    "                metrics.accuracy_score(y_true=actual_labels,\n",
    "                                       y_pred=predicted_labels),2)\n",
    "\n",
    "# Now we calculate 'accuracy' metric manually\n",
    "# For this, we need to declare the counts in each cell... manually!\n",
    "true_positive = 5.\n",
    "false_positive = 6.\n",
    "false_negative = 5.\n",
    "true_negative = 4.\n",
    "\n",
    "accuracy_manual = np.round(\n",
    "                    (true_positive + true_negative) /\n",
    "                      (true_positive + true_negative +\n",
    "                       false_negative + false_positive),2)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Manually computed accuracy:', accuracy_manual)\n",
    "\n",
    "\n",
    "# We calculate 'precision' using the function and compare with our manual calculation\n",
    "precision = np.round(\n",
    "                metrics.precision_score(y_true=actual_labels,\n",
    "                                        y_pred=predicted_labels,\n",
    "                                        pos_label=positive_class),2)\n",
    "precision_manual = np.round(\n",
    "                        (true_positive) /\n",
    "                        (true_positive + false_positive),2)\n",
    "print('Precision:', precision)\n",
    "print('Manually computed precision:', precision_manual)\n",
    "\n",
    "\n",
    "# We calculate 'recall' using the function and compare with our manual calculation\n",
    "recall = np.round(\n",
    "            metrics.recall_score(y_true=actual_labels,\n",
    "                                 y_pred=predicted_labels,\n",
    "                                 pos_label=positive_class),2)\n",
    "recall_manual = np.round(\n",
    "                    (true_positive) /\n",
    "                    (true_positive + false_negative),2)\n",
    "print('Recall:', recall)\n",
    "print('Manually computed recall:', recall_manual)\n",
    "\n",
    "\n",
    "# Finally we calculate 'F1-score' using the function and compare with our manual calculation\n",
    "f1_score = np.round(\n",
    "                metrics.f1_score(y_true=actual_labels,\n",
    "                                 y_pred=predicted_labels,\n",
    "                                 pos_label=positive_class),2) \n",
    "f1_score_manual = np.round(\n",
    "                    (2 * precision * recall) /\n",
    "                    (precision + recall),2)\n",
    "print('F1 score:', f1_score)\n",
    "print('Manually computed F1 score:', f1_score_manual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
