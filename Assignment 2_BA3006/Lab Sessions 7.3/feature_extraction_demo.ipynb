{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n",
      "[[0 0 1 0 0 0 1 0 0]]\n",
      "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n",
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n",
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n",
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n",
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n",
      "[[1.92 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.51 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.92 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.22 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.92 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.22 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.92 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.51]]\n",
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n",
      "[2.5  4.35 3.5  2.89]\n",
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n",
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n",
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n",
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-357ddf1b1958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m                                \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                                \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                                sample=1e-3)\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;31m# Averaging word vectors of a document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created July 2017\n",
    "\n",
    "@author: arw\n",
    "\"\"\"\n",
    "\n",
    "# Let's start with a 'toy' corpus\n",
    "CORPUS = [\n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "\n",
    "# We use new_doc as our test dataset\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print(df)\n",
    "\n",
    "# We pass our CORPUS to the simplest bow extractor we created\n",
    "from feature_extractors import bow_extractor    \n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense() # Since we can't view the default 'sparse matrix'\n",
    "print(features)\n",
    "\n",
    "# Remember, we always need to extract the same features from our test data too!\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(new_doc_features)\n",
    "\n",
    "# Let's see which words/tokens these counts are for...\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# Let's print both the feature names and counts together\n",
    "# - first for the training data and then for the test data\n",
    "display_features(features, feature_names)\n",
    "display_features(new_doc_features, feature_names)\n",
    "\n",
    "\n",
    "# Now let's try the same with tf-idf instead of frequency counts\n",
    "# We use the tfidf_transformer function we defined\n",
    "import numpy as np\n",
    "from feature_extractors import tfidf_transformer\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# We again convert to the dense form to print the values out    \n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)\n",
    "# We do the same for the test document\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)\n",
    "\n",
    "\n",
    "\n",
    "# We can also compute tf-idf scores/vectors ourselves from scratch\n",
    "# - without using sklearn's TfidfTransformer class\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# We compute term frequencies by simply using our bow model\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "\n",
    "# Check if our term frequencies are as expected\n",
    "display_features(tf, feature_names)\n",
    "\n",
    "# We next build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # to smoothen idf later\n",
    "\n",
    "# Check if our document frequencies are as expected\n",
    "display_features([df], feature_names)\n",
    "\n",
    "# Now compute the inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "\n",
    "# Are our inverse document frequencies what we expected?\n",
    "display_features([np.round(idf, 2)], feature_names)\n",
    "\n",
    "# Now compute the idf diagonal matrix  \n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()\n",
    "\n",
    "# Is the idf diagonal matrix as expected?\n",
    "print(np.round(idf, 2))\n",
    "\n",
    "# Now compute the full tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "\n",
    "# Is the tfidf feature matrix what we expected?\n",
    "display_features(np.round(tfidf, 2), feature_names)\n",
    "\n",
    "# Now compute the L2 norms \n",
    "norms = norm(tfidf, axis=1)\n",
    "\n",
    "# Display the L2 norms for each document\n",
    "print(np.round(norms, 2))\n",
    "\n",
    "# Now compute the 'normalized' tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "\n",
    "# Check if the final tfidf feature matrix is as expected\n",
    "# Is it the same as what we got using the TfidfTransformer class of sklearn?\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)\n",
    " \n",
    "\n",
    "# Now do the same for the test data \n",
    "# First, compute the term freqs from bow freqs for the test data - new_doc\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n",
    "\n",
    "# Next compute tfidf using idf matrix from the train corpus\n",
    "nd_tfidf = nd_tf*idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]\n",
    "\n",
    "# Check the new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)\n",
    "\n",
    "\n",
    "\n",
    "# sklearn's TfidfVectorizer provides a transformer to extract tfidf scores directly\n",
    "# from raw data - avoiding the need for CountVectorizer based bow scores\n",
    "from feature_extractors import tfidf_extractor\n",
    "    \n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)\n",
    "\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)    \n",
    "\n",
    "\n",
    "\n",
    "# We can also do more sophisticated word-vector models using Google's word2vec algorithm\n",
    "# using the gensim python package\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in new_doc]                        \n",
    "\n",
    "# Model parameters for the NN-based word2vec 'word embeddings':\n",
    "# size - dimension of the word vectors (tens to thousands)\n",
    "# window - window size to conside the context of a word\n",
    "# min_count - the minimum frequency of a word in the whole corpus to be included in vocabulary\n",
    "# sample - used to downsample the effects of the occurence of frequent words\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, \n",
    "                               size=10,\n",
    "                               window=10,\n",
    "                               min_count=2,\n",
    "                               sample=1e-3)\n",
    "\n",
    "# Averaging word vectors of a document\n",
    "from feature_extractors import averaged_word_vectorizer\n",
    "\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                 model=model.wv,\n",
    "                                                 num_features=10)\n",
    "print(np.round(avg_word_vec_features, 3))\n",
    "\n",
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                    model=model.wv,\n",
    "                                                    num_features=10)\n",
    "print(np.round(nd_avg_word_vec_features, 3))\n",
    "\n",
    "\n",
    "# Using tfidf weighted average of word vectors in a document              \n",
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model.wv, \n",
    "                                                                     num_features=10)\n",
    "print(np.round(wt_tfidf_word_vec_features, 3))\n",
    "\n",
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                                     tfidf_vectors=nd_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model.wv, \n",
    "                                                                     num_features=10)\n",
    "print(np.round(nd_wt_tfidf_word_vec_features, 3))\n",
    "\n",
    "\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
