{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice adventures wonderland lewis carroll 1865\n",
      "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'], ['species', 'african elephant', 'asian elephant'], ['elephants', 'sub-saharan africa', 'south asia', 'southeast asia'], ['male african elephants', 'extant terrestrial animals'], ['elephants', 'long trunk', 'many purposes', 'breathing', 'water', 'grasping objects'], ['incisors', 'tusks', 'weapons', 'tools', 'objects', 'digging'], ['elephants', 'large ear flaps', 'body temperature'], ['pillar-like legs', 'great weight'], ['african elephants', 'ears', 'backs', 'asian elephants', 'ears', 'convex', 'level backs']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('chapter', 1.0),\n",
       " ('rabbit', 1.0),\n",
       " ('thought alice', 1.0),\n",
       " ('sides', 1.0),\n",
       " ('ignorant little girl', 1.0),\n",
       " ('dinah', 1.0),\n",
       " ('thump', 1.0),\n",
       " ('whiskers', 1.0),\n",
       " ('* * * * * * *', 1.0),\n",
       " ('* * * * * *', 1.0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on July 2017\n",
    "\n",
    "@author: arw\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from normalization import normalize_corpus\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "\n",
    "# Load the Alice corpus from NLTK\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "normalized_alice = filter(None, normalize_corpus(alice, lemmatize=False))\n",
    "norm_alice = list(normalized_alice)\n",
    "# Print the first line of the corpus\n",
    "print(norm_alice[0])\n",
    "\n",
    "# Create a single long string out of the corpus\n",
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() \n",
    "                     for document in corpus])\n",
    "                     \n",
    "# The zip function below 'merges' a set of n lists item by item\n",
    "# Try to setup 2 lists of the same length and call zip with the two lists as arguments\n",
    "# Hint: need to cast result to list for viewing contents\n",
    "\n",
    "# See the effect of the following function by calling compute_ngrams([1,2,3,4], 2) and compute_ngrams([1,2,3,4], 3)\n",
    "def compute_ngrams(sequence, n):\n",
    "    return zip(*[sequence[index:] \n",
    "                 for index in range(n)])\n",
    "\n",
    "\n",
    "# We generalize these ideas to get a generic function to get the top n-grams from a corpus\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    # Compute the frequencies of the n-grams using NLTK's FreqDist class\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) \n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams   \n",
    "    \n",
    "# Now try this function for bigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=2,\n",
    "               limit=10)\n",
    "# And for trigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=3,\n",
    "               limit=10)\n",
    "\n",
    "# NLTK has built-in collocation finders can use frequencies of pointwise mutual information (pmi)\n",
    "# Read and understand the intuitive meaning of PMI from the web\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])\n",
    "bigram_measures = BigramAssocMeasures() \n",
    "\n",
    "# Using raw frequencies for collocations                                          \n",
    "finder.nbest(bigram_measures.raw_freq, 10)\n",
    "# Using mutual information scores for collocations\n",
    "finder.nbest(bigram_measures.pmi, 10)   \n",
    "\n",
    "\n",
    "# We can repeat the above for trigrams too\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])\n",
    "trigram_measures = TrigramAssocMeasures()                                                \n",
    "finder.nbest(trigram_measures.raw_freq, 10)\n",
    "finder.nbest(trigram_measures.pmi, 10)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We now use weighted tag based phrase extraction\n",
    "# For this, we need to be able to perform some chunking\n",
    "\n",
    "toy_text = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\"\n",
    "\n",
    "from normalization import parse_document\n",
    "import itertools\n",
    "from normalization import stopword_list\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Extract chunks we are interested in (and omit chinks we are not interested in)\n",
    "# This process depends on the POS tags and grammar tags used in the corpus we want to use\n",
    "# Here anything with a chunk tag 'O' is a chink\n",
    "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}'):\n",
    "    # Build a chunker based on grammar pattern above\n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # POS tag the sentences\n",
    "        tagged_sents = nltk.pos_tag_sents(\n",
    "                            [nltk.word_tokenize(sentence)])\n",
    "        # Extract the chunks\n",
    "        chunks = [chunker.parse(tagged_sent) \n",
    "                  for tagged_sent in tagged_sents]\n",
    "        # Get word, pos tag, chunk tag triples\n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                     for chunk in chunks]    \n",
    "         \n",
    "        flattened_chunks = list(\n",
    "                            itertools.chain.from_iterable(\n",
    "                                wtc_sent for wtc_sent in wtc_sents)\n",
    "                           )\n",
    "        # Get only valid chunks based on tags\n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
    "                        for status, chunk \n",
    "                        in itertools.groupby(flattened_chunks, \n",
    "                                             # get only if chunk != 'O'\n",
    "                                             lambda wdposchnk: wdposchnk[2] != 'O')]\n",
    "        # Append words in each chunk to make phrases\n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                for word, tag, chunk \n",
    "                                in wtc_group \n",
    "                                    if word.lower() \n",
    "                                        not in stopword_list) \n",
    "                                    for status, wtc_group \n",
    "                                    in valid_chunks_tagged\n",
    "                                        if status]\n",
    "        # Append all valid chunked phrases                                    \n",
    "        all_chunks.append(valid_chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "    \n",
    "sentences = parse_document(toy_text)          \n",
    "valid_chunks = get_chunks(sentences)\n",
    "# Print all valid chunks\n",
    "print(valid_chunks)\n",
    "\n",
    "\n",
    "# Build a chunk extractor based on TF-IDF weights instead of frequencies\n",
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    # Get valid chunks as before\n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "    # This time build a tf-idf based model                                 \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    # Get phrases and their tf-idf weights\n",
    "    weighted_phrases = {dictionary.get(id): round(value,3) \n",
    "                        for doc in corpus_tfidf \n",
    "                        for id, value in doc}\n",
    "                            \n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    # Return the top n weighted phrases\n",
    "    return weighted_phrases[:top_n]\n",
    "\n",
    "# Get top 10 tf-idf weighted keyphrases for toy_text\n",
    "get_tfidf_weighted_keyphrases(sentences, top_n=10)\n",
    "\n",
    "# Try with other corpora such as the Alice corpus from NLTK's Guttenburg collection\n",
    "get_tfidf_weighted_keyphrases(alice, top_n=10)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
