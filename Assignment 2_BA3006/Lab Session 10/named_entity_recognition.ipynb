{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated July 2017\\n\\n@author: arw\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created July 2017\n",
    "\n",
    "@author: arw\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "Entity extraction using NLTK's NE Recognizer:\n",
      "          Entity Name   Entity Type\n",
      "0              Munich  ORGANIZATION\n",
      "1                UEFA  ORGANIZATION\n",
      "2              Munich           GPE\n",
      "3           FC Bayern  ORGANIZATION\n",
      "4             Germany           GPE\n",
      "5              Bayern        PERSON\n",
      "6   Franz Beckenbauer        PERSON\n",
      "7          Franz John        PERSON\n",
      "8            European  ORGANIZATION\n",
      "9             Overall           GPE\n",
      "10             Bayern           GPE\n",
      "11             German           GPE\n",
      "12         Bundesliga  ORGANIZATION\n",
      "13            Bavaria           GPE\n",
      "\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at /Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1ebc5c1eb6b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Load stanford NER by pointing to the pre-trained English model and the code (jar file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m sn = StanfordNERTagger('/Users/arw/Documents/Work/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n\u001b[1;32m---> 64\u001b[1;33m                        path_to_jar='/Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar')\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m# First POS-tag the sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\opencv\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\opencv\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     70\u001b[0m             )\n\u001b[0;32m     71\u001b[0m         self._stanford_jar = find_jar(\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\opencv\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    845\u001b[0m     return next(\n\u001b[0;32m    846\u001b[0m         find_jar_iter(\n\u001b[1;32m--> 847\u001b[1;33m             \u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m         )\n\u001b[0;32m    849\u001b[0m     )\n",
      "\u001b[1;32mc:\\users\\user\\opencv\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m             raise LookupError(\n\u001b[1;32m--> 733\u001b[1;33m                 \u001b[1;34m\"Could not find %s jar file at %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m             )\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at /Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "#Sample document for extracting NEs from\n",
    "text = \"\"\"\n",
    "Bayern Munich, or FC Bayern, is a German sports club based in Munich, \n",
    "Bavaria, Germany. It is best known for its professional football team, \n",
    "which plays in the Bundesliga, the top tier of the German football \n",
    "league system, and is the most successful club in German football \n",
    "history, having won a record 26 national titles and 18 national cups. \n",
    "FC Bayern was founded in 1900 by eleven football players led by Franz John. \n",
    "Although Bayern won its first national championship in 1932, the club \n",
    "was not selected for the Bundesliga at its inception in 1963. The club \n",
    "had its period of greatest success in the middle of the 1970s when, \n",
    "under the captaincy of Franz Beckenbauer, it won the European Cup three \n",
    "times in a row (1974-76). Overall, Bayern has reached ten UEFA Champions \n",
    "League finals, most recently winning their fifth title in 2013 as part \n",
    "of a continental treble. \n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "from normalization import parse_document\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenize sentences using our helper functions in normalization.py\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "# POS tag the sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "# Extract all named entities\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "    for tagged_tree in ne_tagged_sentence:\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves())\n",
    "                entity_type = tagged_tree.label()\n",
    "                named_entities.append((entity_name, entity_type))\n",
    "\n",
    "# Get unique named entities - why do this?\n",
    "named_entities = list(set(named_entities))\n",
    "# Store named entities in a (pandas) data frame (for pretty printing)\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])\n",
    "\n",
    "# Display results as a table\n",
    "print(\"Entity extraction using NLTK's NE Recognizer:\")\n",
    "print(entity_frame)   \n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Repeat the process above for the stanford NE Recognizer (Java program wrapped in NLTK)\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "# Set java path in environment variables\n",
    "java_path = r'/usr/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# Load stanford NER by pointing to the pre-trained English model and the code (jar file)\n",
    "sn = StanfordNERTagger('/Users/arw/Documents/Work/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       path_to_jar='/Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar')\n",
    "\n",
    "# First POS-tag the sentences                       \n",
    "ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences]\n",
    "\n",
    "# Then extract the named entities\n",
    "named_entities = []\n",
    "for sentence in ne_annotated_sentences:\n",
    "    temp_entity_name = ''\n",
    "    temp_named_entity = None\n",
    "    for term, tag in sentence:\n",
    "        # Get terms with NE tags\n",
    "        if tag != 'O':\n",
    "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "            temp_named_entity = (temp_entity_name, tag)\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                named_entities.append(temp_named_entity)\n",
    "                temp_entity_name = ''\n",
    "                temp_named_entity = None\n",
    "\n",
    "# Extract the unique named entities - why?\n",
    "named_entities = list(set(named_entities))\n",
    "# Store named entities in a (pandas) data frame as before\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])\n",
    "\n",
    "# Display the results in a table form as before\n",
    "print(\"Entity extraction using StanfordNER's NE Recognizer:\")\n",
    "print(entity_frame)\n",
    "\n",
    "# Compare the results of the 2 NER systems                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
